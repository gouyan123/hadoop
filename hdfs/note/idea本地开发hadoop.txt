---maven中央仓库：search.maven.org，搜 hadoop-client，选org.apache.hadoop单击，
---javaAPI操作hdfs的准备
1、添加apache源，maven默认是不支持apache仓库的，需要在pom.xml中配置apache的仓库
<repositories>
    <repository>
        <id>apache</id>
        <url>http://maven.apache.org</url>
    </repository>
</repositories>
2、添加hadoop依赖，不要引入hadoop-core包，版本太低，会冲突，用hadoop-common，hadoop-hdfs，hadoop-client 3个包即可；
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.7.3</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>2.7.3</version>
</dependency>
<!--客户端操作 hadoop-->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>2.7.3</version>
</dependency>
---
---JavaAPI操作 hdfs分布式文件系统
org.apache.hadoop.fs包中几个重要的类如下：
configuration类：该类的对象封装了配置信息，这些配置信息来自core-*.xml；
FileSystem类：文件系统类，可使用该类的方法对文件/目录进行操作。一般通过FileSystem的静态方法get获得一个文件系统对象；
FSDataInputStream和FSDataOutputStream类：HDFS中的输入输出流。分别通过FileSystem的Open方法和create方法获得；
使用 java api操作hdfs：
创建 cn.edu360.hdfs.javaAPI.HdfsClient类，使用java api操作hdfs；
流实质：根据数据类型，每次读取 n个字节，并将这n个字节转换为相应内容，以此类推；
---采集本地日志数据到 hdfs
日志文件定时上传hdfs：
1、流程
启动一个定时任务：
	——定时探测日志源目录
	——获取需要采集的文件
	——移动这些文件到一个待上传临时目录(防止在原路径下，上传一半，文件发生改变)
	——遍历待上传目录中各文件，逐一传输到HDFS的目标路径，同时将传输完成的文件移动到备份目录
启动一个定时任务：
	——探测备份目录中的备份数据，检查是否已超出最长备份时长，如果超出，则删除

2、规划各种路径
日志源路径： d:/logs/accesslog/
待上传临时目录： d:/logs/toupload/
备份目录： d:/logs/backup/日期/

HDFS存储路径： /logs/日期
HDFS中的文件的前缀：access_log_
HDFS中的文件的后缀：.log
创建 cn.edu360.hdfs.datacollect.DataCollectMain类，创建定时器Timer，周期执行线程任务 TimerTask，该任务负责收集日志并备份；

创建 定时任务 CollectTask，定时去本地目录获取 日志文件，保存到临时目录后，将临时目录中日志文件上传到 hdfs；
日志来源：resources路径下定义了log4j.properties，其中定义日志存放路径 log4j.appender.test1.File = d:/logs/collect/collect.log

创建 定时任务 BackupCleanTask，定时清空备份路径下的文件；

目前功能已实现，但是不规范；1 代码中都是 常量，常量应该定义在 配置文件中，此处定义 collect.properties配置文件；2 定义单例模式类 PropertyHolderLazy，读取配置
文件；
---
---WordCoud简单框架
MapReduce简介：见 mapreduce01.jpg，mapreduce02.jpg
MapReduce分为 2个阶段：
1、map阶段：自动创建若干个MapTask，去读取 HDFS上面的文件，读取hdfs中文件之前，要对所有的要读取文件进行 切片；如果每个切片大小为128M，而要处理的文件总共500M，
则要划分为 4个切片，需启动4个 MapTask，一个 MapTask处理一个任务；当MapTask数量过大时，可以分批进行处理；MapTask每读一行，使用接口进行处理，但是，MapTask并不
知道用户想怎么处理，因此用户必须将实现类提供给 MapTask的接口，并通过配置文件指定要调的类；方法参数：hdfs中取出的数据；最后通过 Context，返回结果；每个MapTask
只拿到其中一部分数据，还需要 reduce阶段；
2、reduce阶段：ReduceTask1将所有MapTask中的key为Hello的数据汇总到一起计算数量，ReduceTask2将所有MapTask中key为 World的数据汇总到一起计算数量；即某一类的数
据给一个 ReduceTask，另一类数据给另一个ReduceTask；即shuffle 数据分发，发牌，将同一类数据发给同一个 ReduceTask进行处理，而是去调一个接口的实现类的方法，实现
类的实现方法由用户自己定义，它会将参数通过接口方法传过来；最后再将结果写到 hdfs的某路径下；

基于mapreduce写WordCount：
各 MapTask先去 hdfs读取数据，然后一行一行的处理数据(写死的)，每拿一行数据，调一下自己实现的 map(long key,String v,context)方法，对行数据进行逻辑处理，其中
key表示行号，v表示行的内容；通过context返回 context.write(单词,1)，例如context.write("Hello",1),context.write("Hello",1),context.write("World",1);
map()处理后，将数据交给 shuffle，shuffle将数据分发给各个 ReduceTask，将key相同的所有数据都要发给同一个ReduceTask，然后调用 reduce(k,value迭代器,context)
对key相同的值进行计算，然后通过 context.write(k,count)返回；
*** 以上为基本原理，下面设计WordCount的框架，见 cn.edu360.hdfs.wordcount；
HdfsWordcount.main()方法为框架主流程，设计后绝对不能修改；框架主流程中 只能使用接口，不能使用 实现类，因为设计这个主流程的时候，还没有实现类，实现类是以后开发
的时候实现具体功能的；例如，HdfsWordcount.main()这个框架主流程中使用 Mapper接口，实现类是开发的时候写的，分别为CaseIgnorWcountMapper和WordCountMapper，并
将这 2个实现类配在 配置文件 job.properties中，框架主流程 通过反射进行实例化，并 赋值给Mapper接口；这样，当新定义Mapper接口实现类的时候，只需要将实现类写在
job.properties配置文件中即可；
---
基于mapreduce实现 WordCount：具体原理看 有道云笔记 day03-MapReduce与Yarn详解
创建 WordcountMapper extents Mapper，maptask每从hdfs中读取一条数据，就调用一次这里的 map()方法；
public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
   	@Override
   	protected void map(LongWritable key, Text value, Context context)
   			throws IOException, InterruptedException {
   		// 切单词
   		String line = value.toString();
   		String[] words = line.split(" ");
   		for(String word:words){
   			context.write(new Text(word), new IntWritable(1));
   		}
   	}
}
创建 WordcountReducer extents Reducer，reducetask接收到一组数据，就调用一次reduce()方法；一组的含义：('Hello',1),('Hello',2),('Hello',3)
public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
		int count = 0;
		Iterator<IntWritable> iterator = values.iterator();
		while(iterator.hasNext()){
			IntWritable value = iterator.next();
			count += value.get();
		}
		context.write(key, new IntWritable(count));
	}
}
创建 JobSubmitter类，将自定义的WordcountMapper类和WordcountReducer类提交到 yarn集群上面运行；
1)将工程整体打成一个jar包并上传到linux机器上；
2)准备好要处理的数据文件放到hdfs的指定目录中；
3)用命令启动jar包中的Jobsubmitter，让它去提交jar包给yarn来运行其中的mapreduce程序  ：  hadoop jar wc.jar cn.edu360.mr.wordcount.JobSubmitter .....；
4)去hdfs的输出目录中查看结果；
public class JobSubmitter {
	public static void main(String[] args) throws Exception {
		// 在代码中设置JVM系统参数，用于给job对象来获取访问HDFS的用户身份
		System.setProperty("HADOOP_USER_NAME", "root");
		Configuration conf = new Configuration();
		// 1、设置job运行时要访问的默认文件系统
		conf.set("fs.defaultFS", "hdfs://h250:9000");
		// 2、设置job提交到哪去运行；1 在yarn上运行；2 在本地运行；
		conf.set("mapreduce.framework.name", "yarn");
		// 设置 yarn的 Resource Manager节点的 hostname；
		conf.set("yarn.resourcemanager.hostname", "h250");
		// 3、如果要从windows系统上运行这个job提交客户端程序，则需要加这个跨平台提交的参数
		conf.set("mapreduce.app-submission.cross-platform","true");
		// 根据配置获取 job对象
		Job job = Job.getInstance(conf);
		// 1、job中封装参数：jar包所在的位置
//		job.setJar("d:/wc.jar");	// 这种写死了，一般不用
		// 通过JobSubmitter.class找到它的包的路径；
		job.setJarByClass(JobSubmitter.class);
		// 2、封装参数： 本次job所要调用的Mapper实现类、Reducer实现类
		job.setMapperClass(WordcountMapper.class);
		job.setReducerClass(WordcountReducer.class);
		// 3、封装参数：本次job的Mapper实现类、Reducer实现类产生的结果数据的key、value类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		Path output = new Path("/wordcount/output");
		FileSystem fs = FileSystem.get(new URI("hdfs://h250:9000"),conf,"root");
		if(fs.exists(output)){
			fs.delete(output, true);
		}
		// 4、封装参数：本次job要处理的输入数据集所在路径、最终结果的输出路径
		FileInputFormat.setInputPaths(job, new Path("/wordcount/input"));
		FileOutputFormat.setOutputPath(job, output);  // 注意：输出路径必须不存在
		// 5、封装参数：想要启动的reduce task的数量；map task数量由系统分配；
		job.setNumReduceTasks(2);
		// 6、提交job给yarn，mapreduce程序在 yarn中运行，客户端会与mapreduce程序失去联系；因此使用 job.waitForCompletion()方法，表示客户端一直等待，不退出；
		// true表示将 ResoureManager返回的信息打印出来；
		boolean res = job.waitForCompletion(true);
		System.exit(res?0:-1);
	}
}


