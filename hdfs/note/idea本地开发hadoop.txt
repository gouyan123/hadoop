---javaAPI操作hdfs的准备
1、添加apache源，maven默认是不支持apache仓库的，需要在pom.xml中配置apache的仓库
<repositories>
    <repository>
        <id>apache</id>
        <url>http://maven.apache.org</url>
    </repository>
</repositories>
2、添加hadoop依赖，不要引入hadoop-core包，版本太低，会冲突，用hadoop-common，hadoop-hdfs，hadoop-client 3个包即可；
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.7.3</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>2.7.3</version>
</dependency>
<!--客户端操作 hadoop-->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>2.7.3</version>
</dependency>
---
---JavaAPI操作 hdfs分布式文件系统
org.apache.hadoop.fs包中几个重要的类如下：
configuration类：该类的对象封装了配置信息，这些配置信息来自core-*.xml；
FileSystem类：文件系统类，可使用该类的方法对文件/目录进行操作。一般通过FileSystem的静态方法get获得一个文件系统对象；
FSDataInputStream和FSDataOutputStream类：HDFS中的输入输出流。分别通过FileSystem的Open方法和create方法获得；
使用 java api操作hdfs：
创建 cn.edu360.hdfs.javaAPI.HdfsClient类，使用java api操作hdfs；
流实质：根据数据类型，每次读取 n个字节，并将这n个字节转换为相应内容，以此类推；
---采集本地日志数据到 hdfs
日志文件定时上传hdfs：
1、流程
启动一个定时任务：
	——定时探测日志源目录
	——获取需要采集的文件
	——移动这些文件到一个待上传临时目录(防止在原路径下，上传一半，文件发生改变)
	——遍历待上传目录中各文件，逐一传输到HDFS的目标路径，同时将传输完成的文件移动到备份目录
启动一个定时任务：
	——探测备份目录中的备份数据，检查是否已超出最长备份时长，如果超出，则删除

2、规划各种路径
日志源路径： d:/logs/accesslog/
待上传临时目录： d:/logs/toupload/
备份目录： d:/logs/backup/日期/

HDFS存储路径： /logs/日期
HDFS中的文件的前缀：access_log_
HDFS中的文件的后缀：.log
创建 cn.edu360.hdfs.datacollect.DataCollectMain类，创建定时器Timer，周期执行线程任务 TimerTask，该任务负责收集日志并备份；

创建 定时任务 CollectTask，定时去本地目录获取 日志文件，保存到临时目录后，将临时目录中日志文件上传到 hdfs；
日志来源：resources路径下定义了log4j.properties，其中定义日志存放路径 log4j.appender.test1.File = d:/logs/collect/collect.log

创建 定时任务 BackupCleanTask，定时清空备份路径下的文件；

目前功能已实现，但是不规范；1 代码中都是 常量，常量应该定义在 配置文件中，此处定义 collect.properties配置文件；2 定义单例模式类 PropertyHolderLazy，读取配置
文件；
---
---WordCoud简单框架
MapReduce简介：见 mapreduce01.jpg，mapreduce02.jpg
MapReduce分为 2个阶段：
1、map阶段：自动创建若干个MapTask，去读取 HDFS上面的文件，读取hdfs中文件之前，要对所有的要读取文件进行 切片；如果每个切片大小为128M，而要处理的文件总共500M，
则要划分为 4个切片，需启动4个 MapTask，一个 MapTask处理一个任务；当MapTask数量过大时，可以分批进行处理；MapTask每读一行，使用接口进行处理，但是，MapTask并不
知道用户想怎么处理，因此用户必须将实现类提供给 MapTask的接口，并通过配置文件指定要调的类；方法参数：hdfs中取出的数据；最后通过 Context，返回结果；每个MapTask
只拿到其中一部分数据，还需要 reduce阶段；
2、reduce阶段：ReduceTask1将所有MapTask中的key为Hello的数据汇总到一起计算数量，ReduceTask2将所有MapTask中key为 World的数据汇总到一起计算数量；即某一类的数
据给一个 ReduceTask，另一类数据给另一个ReduceTask；即shuffle 数据分发，发牌，将同一类数据发给同一个 ReduceTask进行处理，而是去调一个接口的实现类的方法，实现
类的实现方法由用户自己定义，它会将参数通过接口方法传过来；最后再将结果写到 hdfs的某路径下；

基于mapreduce写WordCount：
各 MapTask先去 hdfs读取数据，然后一行一行的处理数据(写死的)，每拿一行数据，调一下自己实现的 map(long key,String v,context)方法，对行数据进行逻辑处理，其中
key表示行号，v表示行的内容；通过context返回 context.write(单词,1)，例如context.write("Hello",1),context.write("Hello",1),context.write("World",1);
map()处理后，将数据交给 shuffle，shuffle将数据分发给各个 ReduceTask，将key相同的所有数据都要发给同一个ReduceTask，然后调用 reduce(k,value迭代器,context)
对key相同的值进行计算，然后通过 context.write(k,count)返回；
*** 以上为基本原理，下面设计WordCount的框架，见 cn.edu360.hdfs.wordcount；
HdfsWordcount.main()方法为框架主流程，设计后绝对不能修改；框架主流程中 只能使用接口，不能使用 实现类，因为设计这个主流程的时候，还没有实现类，实现类是以后开发
的时候实现具体功能的；例如，HdfsWordcount.main()这个框架主流程中使用 Mapper接口，实现类是开发的时候写的，分别为CaseIgnorWcountMapper和WordCountMapper，并
将这 2个实现类配在 配置文件 job.properties中，框架主流程 通过反射进行实例化，并 赋值给Mapper接口；这样，当新定义Mapper接口实现类的时候，只需要将实现类写在
job.properties配置文件中即可；
---
